{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {
        "id": "Yc_QGcPrvhgj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from math import e"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Método forward\n",
        "\n",
        "Se encarga de devolver el resultado de evaluar la función $f_\\theta$.\n",
        "$$f_\\theta = W_2 \\sigma (W_1 x + b_1)+b_2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 522,
      "metadata": {
        "id": "Zz81ZoDxvhgm"
      },
      "outputs": [],
      "source": [
        "def forward(W1, W2, b1, b2, x):\n",
        "    capa1 = W1 @ x + b1\n",
        "    \n",
        "    sigmoide = 1/(1+np.exp(-capa1))\n",
        "\n",
        "    capa2 = W2 @ sigmoide + b2\n",
        "    return capa2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Calculo del gradiente numerico\n",
        "def funcion_objetivo(x, y, W1, W2, b1, b2):\n",
        "    loss = 0.5*np.mean((forward(W1,W2,b1,b2,x.transpose())-y)**2)\n",
        "    return loss\n",
        "\n",
        "def numerical_gradient(x, y, W1, W2, b1, b2, eps):\n",
        "    for i in range(W1.shape[0]):\n",
        "        for j in range(W1.shape[1]):\n",
        "            W1_izq = W1 \n",
        "            W1_izq[i][j] = W1_izq[i][j] + eps\n",
        "            W1_der = W1 \n",
        "            W1_der[i][j] = W1_der[i][j] - eps\n",
        "            izq_W1 = funcion_objetivo(x, y, W1_izq, W2, b1, b2)\n",
        "            der_W1 = funcion_objetivo(x, y, W1_der, W2, b1, b2)\n",
        "            derivada_W1 = (izq_W1 - der_W1)/(2*eps)\n",
        "    for i in range(W2.shape[0]):\n",
        "        for j in range(W2.shape[1]):\n",
        "            W2_izq = W2\n",
        "            W2_izq[i][j] = W2_izq[i][j] + eps\n",
        "            W2_der = W2\n",
        "            W2_der[i][j] = W2_der[i][j] - eps\n",
        "            izq_W2 = funcion_objetivo(x, y, W1, W2_izq, b1, b2)\n",
        "            der_W2 = funcion_objetivo(x, y, W1, W2_der, b1, b2)\n",
        "            derivada_W2 = (izq_W2 - der_W2)/(2*eps)\n",
        "    for i in range(b1.shape[0]):\n",
        "        for j in range(b1.shape[1]):\n",
        "            b1_izq = b1\n",
        "            b1_izq[i][j] = b1_izq[i][j] + eps\n",
        "            b1_der = b1\n",
        "            b1_der[i][j] = b1_der[i][j] - eps\n",
        "            izq_b1 = funcion_objetivo(x, y, W1, W2, b1_izq, b2)\n",
        "            der_b1 = funcion_objetivo(x, y, W1, W2, b1_der, b2)\n",
        "            derivada_b1 = (izq_b1 - der_b1)/(2*eps)\n",
        "    for i in range(b2.shape[0]):\n",
        "        for j in range(b2.shape[1]):\n",
        "            b2_izq = b2\n",
        "            b2_izq[i][j] = b2_izq[i][j] + eps\n",
        "            b2_der = b2\n",
        "            b2_der[i][j] = b2_der[i][j] - eps\n",
        "            izq_b2 = funcion_objetivo(x, y, W1, W2, b1, b2_der)\n",
        "            der_b2 = funcion_objetivo(x, y, W1, W2, b1, b2_izq)\n",
        "            derivada_b2 = (izq_b2 - der_b2)/(2*eps)\n",
        "    \n",
        "    gradiente = np.zeros((4,1))\n",
        "    gradiente[0][0] = derivada_W1\n",
        "    gradiente[1][0] = derivada_W2\n",
        "    gradiente[2][0] = derivada_b1\n",
        "    gradiente[3][0] = derivada_b2\n",
        "    \n",
        "    return gradiente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "metadata": {
        "id": "P-R0FjEzvhgn"
      },
      "outputs": [],
      "source": [
        "# #Calculo del gradiente numerico\n",
        "# def funcion_objetivo(x, y, W1, W2, b1, b2):\n",
        "#     loss = 0.5*((forward(W1,W2,b1,b2,x.transpose())-y)**2)\n",
        "#     return loss\n",
        "\n",
        "# def numerical_gradient(x, y, W1, W2, b1, b2, eps):\n",
        "\n",
        "#     cincremental_der_W1 = funcion_objetivo(x, y, W1+eps, W2, b1, b2)\n",
        "#     cincremental_izq_W1 = funcion_objetivo(x, y, W1-eps, W2, b1, b2)\n",
        "#     derivada_W1 = (cincremental_der_W1-cincremental_izq_W1)/(2*eps)\n",
        "\n",
        "#     cincremental_der_W2 = funcion_objetivo(x, y, W1, W2+eps, b1, b2)\n",
        "#     cincremental_izq_W2 = funcion_objetivo(x, y, W1, W2-eps, b1, b2)\n",
        "#     derivada_W2 = (cincremental_der_W2-cincremental_izq_W2)/(2*eps)\n",
        "\n",
        "#     cicremental_der_b1 = funcion_objetivo(x, y, W1, W2, b1 + eps, b2)\n",
        "#     cicremental_izq_b1 = funcion_objetivo(x, y, W1, W2, b1 - eps, b2)\n",
        "#     derivada_b1 = (cicremental_der_b1 - cicremental_izq_b1)/(2*eps)\n",
        "\n",
        "#     cicremental_der_b2 = funcion_objetivo(x, y, W1, W2, b1, b2 + eps)\n",
        "#     cicremental_izq_b2 = funcion_objetivo(x, y, W1, W2, b1, b2 - eps)\n",
        "#     derivada_b2 = (cicremental_der_b2 - cicremental_izq_b2)/(2*eps)\n",
        "\n",
        "#     gradiente = np.zeros((4,1))\n",
        "#     gradiente[0][0] = derivada_W1\n",
        "#     gradiente[1][0] = derivada_W2\n",
        "#     gradiente[2][0] = derivada_b1\n",
        "#     gradiente[3][0] = derivada_b2\n",
        "    \n",
        "#     return gradiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 525,
      "metadata": {
        "id": "rJRtIYxMvhgo"
      },
      "outputs": [],
      "source": [
        "# funcion fit y loop de entrenamiento\n",
        "def fit(x, y, W1, W2, b1, b2, learning_rate, epochs):\n",
        "    eps = 1e-3\n",
        "    loss_accum = []\n",
        "    j = 0\n",
        "    for i in range(epochs):\n",
        "        aux_loss = 0\n",
        "        for j in range(x.shape[0]):\n",
        "            # x_aux = np.array([x[j,:]])\n",
        "            objective_value = funcion_objetivo(x, y, W1, W2, b1, b2)\n",
        "            aux_loss = aux_loss + objective_value\n",
        "\n",
        "            gradiente = numerical_gradient(x, y, W1, W2, b1, b2, eps)\n",
        "            W1 = W1 - learning_rate*gradiente[0]\n",
        "            W2 = W2 - learning_rate*gradiente[1]\n",
        "            b1 = b1 - learning_rate*gradiente[2]\n",
        "            b2 = b2 - learning_rate*gradiente[3]\n",
        "        \n",
        "        loss_accum.append(aux_loss/x.shape[0])\n",
        "\n",
        "    theta = [W1, W2, b1, b2]\n",
        "    return theta, loss_accum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 526,
      "metadata": {},
      "outputs": [],
      "source": [
        "#funcion fit y loop de entrenamiento\n",
        "\n",
        "# def fit2(x, y, W1, W2, b1, b2, learning_rate, epochs):\n",
        "#     eps = 1e-3\n",
        "#     loss_accum = []\n",
        "#     j = 0\n",
        "#     grad_w1 = np.zeros(W1.shape)\n",
        "#     grad_w2 = np.zeros(W2.shape)\n",
        "#     grad_b1 = np.zeros(b1.shape)\n",
        "#     grad_b2 = np.zeros(b2.shape)\n",
        "#     for i in range(epochs):\n",
        "#         aux_loss = 0\n",
        "#         for j in range(x.shape[0]):\n",
        "#             # x_aux = np.array([x[j,:]])\n",
        "\n",
        "#             objective_value = funcion_objetivo(x, y, W1, W2, b1, b2)\n",
        "#             print(objective_value)\n",
        "\n",
        "#             aux_loss = aux_loss + objective_value\n",
        "\n",
        "#             gradiente = numerical_gradient(x, y, W1, W2, b1, b2, eps)\n",
        "#             grad_w1 = grad_w1 + gradiente[0]\n",
        "#             grad_w2 = grad_w2 + gradiente[1]\n",
        "#             grad_b1 = grad_b1 + gradiente[2]\n",
        "#             grad_b2 = grad_b2 + gradiente[3]\n",
        "\n",
        "#         loss_accum.append(aux_loss/x.shape[0])\n",
        "\n",
        "#         grad_w1 = grad_w1 / x.shape[0]\n",
        "#         grad_w2 = grad_w2 / x.shape[0]\n",
        "#         grad_b1 = grad_b1 / x.shape[0]\n",
        "#         grad_b2 = grad_b2 / x.shape[0]\n",
        "\n",
        "#         W1 = W1 - learning_rate * grad_w1\n",
        "#         W2 = W2 - learning_rate * grad_w2\n",
        "#         b1 = b1 - learning_rate * grad_b1\n",
        "#         b2 = b2 - learning_rate * grad_b2\n",
        "\n",
        "#     theta = [W1, W2, b1, b2]\n",
        "#     return theta, loss_accum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 527,
      "metadata": {
        "id": "IS4sKd_Hvhgo"
      },
      "outputs": [],
      "source": [
        "def predict(x, W1, W2, b1, b2):\n",
        "    y = forward(W1, W2, b1, b2, x)\n",
        "    return y"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inicialización de variables W1, b1, W2 y b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 528,
      "metadata": {},
      "outputs": [],
      "source": [
        "W1 = np.random.random((5, 6))  # matriz 5x6 \n",
        "b1 = np.random.random((5, 1))  # vector 5x1\n",
        "\n",
        "W2 = np.random.random((1, 5))  # matriz 1x5\n",
        "b2 = np.random.random((1, 1))  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recolección de datos\n",
        "\n",
        "Tomamos los datos del archivo y los asignamos desde la posición 0 a la 315 como los de entrenamiento, y desde la 315 hasta el final como los datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 529,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"Real estate valuation data set.xlsx\")\n",
        "datos = df.to_numpy()\n",
        "\n",
        "X = datos[:100, [1,2,3,4,5,6]] \n",
        "\n",
        "Y = datos[:100,7]\n",
        "\n",
        "X_test = datos[380:, [1,2,3,4,5,6]]\n",
        "Y_test = datos[380:,7]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalizamos las matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 530,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalizar(x):\n",
        "    for i in range(x.shape[0]):\n",
        "        x[i] = (x[i] - x[i].mean())/x[i].std()\n",
        "    return x  \n",
        "\n",
        "x_norm = normalizar(X)\n",
        "x_test_norm = normalizar(X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenamiento de la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 531,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([[0.64649177, 0.6622452 , 0.2375443 , 0.22669231, 0.97589479,\n",
            "        0.22030988],\n",
            "       [0.2552515 , 0.22116431, 0.08987811, 0.60804677, 0.87922803,\n",
            "        0.95395824],\n",
            "       [0.7124484 , 0.80980624, 0.49973341, 0.96324745, 0.48190757,\n",
            "        0.38305538],\n",
            "       [0.24022764, 0.80565456, 0.56657857, 0.52035165, 0.06372956,\n",
            "        0.50906275],\n",
            "       [0.81670133, 0.17037418, 0.34049605, 0.07731467, 0.08812219,\n",
            "        0.58570331]]), array([[0.43075157, 0.52493089, 0.98822782, 0.80795605, 0.4635443 ]]), array([[0.16137154],\n",
            "       [0.83039321],\n",
            "       [0.26220326],\n",
            "       [0.00097207],\n",
            "       [0.2637235 ]]), array([[0.47094654]])]\n",
            "[745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926, 745.504548475926]\n",
            "[[2.27086548 2.29101299 2.07106812 2.2440731  2.065101   2.29850914\n",
            "  2.28268008 2.10684346 2.10054686 2.29550913 2.26140733 2.11375333\n",
            "  2.25355719 2.18846448 2.06508656 2.24251002 2.19526241 2.16852149\n",
            "  2.12652519 2.28598935 2.24693702 2.10333985 2.28372956 2.28944915\n",
            "  2.27108918 2.29246038 2.25901748 2.09457406 2.10396884 2.06488181\n",
            "  2.29620447 2.25818997 2.29430104 2.29629826]]\n"
          ]
        }
      ],
      "source": [
        "res = fit(X, Y, W1, W2, b1, b2, 0.0005, 1000)\n",
        "theta = res[0]\n",
        "loss = res[1]\n",
        "\n",
        "print(theta)\n",
        "print(loss)\n",
        "\n",
        "xt_test_norm = x_test_norm.transpose()\n",
        "\n",
        "Xt_test = X_test.transpose()\n",
        "\n",
        "predictions = []\n",
        "resta = []\n",
        "\n",
        "preds = forward(theta[0], theta[1], theta[2], theta[3], xt_test_norm)\n",
        "\n",
        "print(preds)\n",
        "\n",
        "# for i in range(X_test.shape[0]): \n",
        "#     predictions.append(predict(xt_test_norm[:,i], theta[0], theta[1], theta[2], theta[3]))\n",
        "#     resta.append(predictions[i] - Y_test[i])\n",
        "#     print(predictions[i], \"- \", Y_test[i], \"=\", resta[i])\n",
        "\n",
        "# for i in range(X_test.shape[0]): \n",
        "#     predictions.append(predict(Xt_test[:,i], theta[0], theta[1], theta[2], theta[3]))\n",
        "#     resta.append(predictions[i] - Y_test[i])\n",
        "#     print(predictions[i], \"- \", Y_test[i], \"=\", resta[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 532,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data = {'epochs': range(1000),\n",
        "#         'loss_acum': loss_acum}\n",
        "\n",
        "# df = pd.DataFrame(data)\n",
        "\n",
        "# df.plot(x='epochs', y='loss_acum')\n",
        "\n",
        "# plt.title('Funcion objetivo a lo largo del entrenamiento')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Loss Acumulated')\n",
        "\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "metodos",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
